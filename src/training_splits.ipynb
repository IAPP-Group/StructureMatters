{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63756d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "\n",
    "DATASET_INFO = {\n",
    "    \"premier\": {\n",
    "        \"name\": \"Premier\",\n",
    "        \"classes\": [\"social\", \"ffmpeg\", \"avidemux\"],\n",
    "        \"videos-path\": \"/Prove/Shullani/GNN-Video-Features/Premier/\",\n",
    "        \"graph-training\": \"./graph-training/\"\n",
    "    }\n",
    "}\n",
    "dataset_name = \"premier\"\n",
    "GRAPHS = \"/Prove/Shullani/GNN-Video-Features/small-graphs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f64dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_collection(txt_filename, video_list):                                      \n",
    "    with open(txt_filename, \"w\") as f:                                                                                                      \n",
    "        for item in video_list:  \n",
    "            f.write(item)\n",
    "            f.write('\\n')\n",
    "\n",
    "def write_csv_collection(csv_filename, video_list):                                          \n",
    "    with open(csv_filename, \"w\") as f:                                                 \n",
    "        writer = csv.writer(f)                                                           \n",
    "        for item in video_list:                                                          \n",
    "            writer.writerow(item)  \n",
    "    \n",
    "def get_row(path):\n",
    "    dirs_split = path.split(os.sep)\n",
    "    video_name = dirs_split[-1].replace(\".bin\", \"\")\n",
    "    label = dirs_split[-2]\n",
    "    return \"{}{}{}\".format(label, os.sep, video_name)\n",
    "\n",
    "\n",
    "def get_split(data_set):\n",
    "    data = []\n",
    "    for item in data_set:\n",
    "        data.append(get_row(item))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_all_files(graphs_path, dataset_name, experiment_name=\"social\"):\n",
    "    premier = glob.glob(os.path.join(graphs_path, dataset_name, experiment_name, \"**/*.bin\"))\n",
    "    device_list = []\n",
    "    for item in premier:\n",
    "        device_list.append(os.path.basename(item).split(\"_\")[0])\n",
    "    device_list = set(device_list)\n",
    "    return premier, device_list\n",
    "\n",
    "\n",
    "def get_train_test_files(graphs_path, dataset_name, experiment_name=\"social\"):\n",
    "    test_set = {}\n",
    "    train_set = {}\n",
    "    premier, device_list = get_all_files(graphs_path, dataset_name, experiment_name)\n",
    "    for dev in device_list:\n",
    "        test_set[dev] = glob.glob(os.path.join(graphs_path, dataset_name, experiment_name, \"**/{}_*.bin\".format(dev)))\n",
    "        # build train\n",
    "        train = set(premier) - set(test_set[dev])\n",
    "        train_set[dev] = list(train)\n",
    "        print(f\"{dev}| test: {len(test_set[dev])}| train: {len(train_set[dev])}\")\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def get_train_test_valid_files(graphs_path, dataset_name, valid_perc=0.1, experiment_name=\"social\"):\n",
    "    test_set = {}\n",
    "    train_set = {}\n",
    "    valid_set = {}\n",
    "    premier, device_list = get_all_files(graphs_path, dataset_name, experiment_name)\n",
    "    valid_devices = math.floor(len(device_list)*valid_perc)\n",
    "    for dev in device_list:\n",
    "        test_set[dev] = glob.glob(os.path.join(graphs_path, dataset_name, experiment_name, \"**/{}_*.bin\".format(dev)))\n",
    "\n",
    "        # build valid\n",
    "        valid_dev_list = list(device_list)\n",
    "        valid_dev_list.remove(dev)\n",
    "        random.shuffle(valid_dev_list)\n",
    "        valid_set[dev] = []\n",
    "        for v_dev in valid_dev_list[:valid_devices]:\n",
    "            tmp = glob.glob(os.path.join(graphs_path, dataset_name, experiment_name, \"**/{}_*.bin\".format(v_dev)))\n",
    "            valid_set[dev].extend(tmp)\n",
    "        \n",
    "        # build train\n",
    "        train = set(premier) - set(test_set[dev]) - set(valid_set[dev])\n",
    "        train_set[dev] = list(train)\n",
    "        print(f\"{dev}| test: {len(test_set[dev])}| train: {len(train_set[dev])} | valid: {len(valid_set[dev])}\")\n",
    "    return train_set, test_set, valid_set\n",
    "\n",
    "\n",
    "def write_split(output_path, data_set, set_type, experiment_name=\"social\"):\n",
    "     for dev in data_set.keys():\n",
    "            data = get_split(data_set[dev])\n",
    "            data_path = os.path.join(output_path, \"{}_{}_{}.txt\".format(dev, experiment_name, set_type))\n",
    "            print(data_path)\n",
    "            write_collection(data_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656d15a",
   "metadata": {},
   "source": [
    "# Build Train/Valid/Test leave-one-device-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2361daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = DATASET_INFO[dataset_name]\n",
    "splits_dict = {i_class:{\"train\":[], \"valid\":[], \"test\":[]} for i_class in data_info[\"classes\"]}\n",
    "for i_class in data_info[\"classes\"]:\n",
    "    print(i_class, \"-------------------------------------\")\n",
    "    splits_dict[i_class][\"train\"], splits_dict[i_class][\"test\"], splits_dict[i_class][\"valid\"] = get_train_test_valid_files(GRAPHS, dataset_name= data_info[\"name\"], experiment_name=i_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696e724",
   "metadata": {},
   "source": [
    "# write splits to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"premier\":\n",
    "    for i_class in data_info[\"classes\"]:\n",
    "        output_path = os.path.join(data_info[\"videos-path\"], i_class)\n",
    "        if i_class == \"social\":\n",
    "            write_split(output_path, splits_dict[i_class][\"train\"], \"train\", experiment_name=i_class)\n",
    "            write_split(output_path, splits_dict[i_class][\"test\"], \"test\", experiment_name=i_class)\n",
    "            write_split(output_path, splits_dict[i_class][\"valid\"], \"valid\", experiment_name=i_class)\n",
    "        else:\n",
    "            write_split(output_path, splits_dict[\"social\"][\"train\"], \"train\", experiment_name=i_class)\n",
    "            write_split(output_path, splits_dict[i_class][\"test\"], \"test\", experiment_name=i_class)\n",
    "            write_split(output_path, splits_dict[\"social\"][\"valid\"], \"valid\", experiment_name=i_class)\n",
    "else:\n",
    "    for i_class in data_info[\"classes\"]:\n",
    "        output_path = os.path.join(data_info[\"videos-path\"], i_class)\n",
    "        write_split(output_path, splits_dict[i_class][\"train\"], \"train\", experiment_name=i_class)\n",
    "        write_split(output_path, splits_dict[i_class][\"test\"], \"test\", experiment_name=i_class)\n",
    "        write_split(output_path, splits_dict[i_class][\"valid\"], \"valid\", experiment_name=i_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c58c6",
   "metadata": {},
   "source": [
    "# build graph premier training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = {i_class:[] for i_class in data_info[\"classes\"]}\n",
    "for key in test_paths.keys():\n",
    "    local_path = os.path.join(data_info[\"videos-path\"], key, \"*_test.txt\")\n",
    "    # collect all test.txt files\n",
    "    test_paths[key] = glob.glob(local_path)\n",
    "    local_class = []\n",
    "    for item in sorted(test_paths[key]):\n",
    "        test = os.path.basename(item).split(\".\")[0]\n",
    "        train = test.replace(\"_test\", \"_train\")\n",
    "        valid = test.replace(\"_test\", \"_valid\")\n",
    "        local_class.append([data_info[\"name\"]+\"-\"+key, train, test, valid])\n",
    "    write_csv_collection(os.path.join(data_info[\"graph-training\"],f\"graph_{dataset_name}_{key}_training.csv\"), local_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b8136-6689-4650-98cc-d608f57bc0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
